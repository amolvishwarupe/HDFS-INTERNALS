1. HDFS is built around the idea that data is written _____but read
many times.
	a) many
	b) twice
	c) data already exists
	d) once
	
	Answer : D

2. Hadoop divides input into fixed size pieces called what?
	a) output result
	b) input splits
	c) input data
	d) input blogs
	
	Answer : B
	
3. All the blocks are replicated in other nodes for ______
	a) security
	b) big data
	c) pool
	d) fault tolerance
	
	Answer : D

4. Block size can be changed using the properties in ______
	a) core-site.xml
	b) Hadoop-env.sh
	c) hdfs-site.xml
	d) yarn-site.xml
	
	Answer:B
	
5. Hadoop uses the ______representation of the data stored in the
file blocks known as Input splits.
	a) physical
	b) logical
	c) mechanical
	d) none
	
	Answer: B
	
6. DFS calls NameNode to create file in file systemâ€™s_____
	a) dataspace
	b) resourcespace
	c) namespace
	d) nodespace
	
	Answer:C
	
7. Data packets are streamed to first DataNode in the ________
	a) handshake
	b) pipeline
	c) hard disk
	d) hdfs
	
	Answer: D
	
8. The client has finished writing data, it calls _______on the stream.
	a) close()
	b) read()
	c) open()
	d) check()
	
	Answer: A

9. Blocks are read in order, with the _________ opening new
connections to datanodes as the client reads through the stream.
	a) DFSoutputstream
	b) DFSInputStream
	c) DFStrackManager
	d) DFSStringConcatination
	
	Answer :B
	
10. If I have 100 input splits, how many maps will run?
	a) 200
	b) 50
	c) 100
	d) 1

	Answer: C